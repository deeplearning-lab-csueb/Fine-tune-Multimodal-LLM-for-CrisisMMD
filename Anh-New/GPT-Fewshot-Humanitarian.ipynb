{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19b8a00-995d-4bdc-9ced-10d3f52095c3",
   "metadata": {},
   "source": [
    "# PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1b1956-5c90-47b9-990a-b976fef20a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMA_TEXT_ONLY_SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI model that classifies text into one of five categories based on its humanitarian relevance in a crisis. Your task is to analyze the given text and assign it the most appropriate category.\n",
    "\n",
    "Your classification should prioritize identifying texts that provide useful crisis-related information, even if they are brief, but you should consider the meaning of the text as a whole and should not just focus on individual keywords or hashtags within the text. For example, if the text only contains a hashtag about donation, but the text is not relevant to a crisis, then it should be categorized as 'not humanitarian'.\n",
    "\n",
    "Please note that if the text is not relevant to a crisis, it should be labeled as 'not humanitarian'. On the other hand, if the text mentions about a crisis (flood, fire, earthquake, storm, hurricane...) then it should be categorized as either 0, 1, 2, or 3. When a text is relevant to a crisis but does not mention to either people, donation or volunteer, and property damage then you can put it into category 3.\n",
    "\n",
    "Classify the text delimited by triple quotes (\\\"\\\"\\\" \\\"\\\"\\\") into one of the following categories:\n",
    "\n",
    "- **0 (affected individuals):** The text provides any relevant information about people impacted by the crisis, including injured, missing, founding, or deceased individuals. In short, if it is about a crisis and related to humans but not related to rescue, volunteering, or donation activities, then it should be labeled as 0.\n",
    "\n",
    "- **1 (rescue, volunteering, or donation efforts):** The text provides any relevant information about rescue operations, volunteering, or donations in a crisis.\n",
    "\n",
    "- **2 (infrastructure and utility damage):** The text provides any relevant information about damage to buildings, roads, power lines, other essential utilities, and vehicles in a crisis. This includes interruptions of utility services or infrastructure during a crisis.\n",
    "\n",
    "- **3 (other relevant information):** The text provides any relevant details about a crisis that do not fit into the categories 0, 1, and 2 above. In short, if it is about a crisis but is not related to human impact, rescue, volunteering, donation activities, or property or vehicle damage, then it should be labeled as category 3.\n",
    "\n",
    "- **4 (not humanitarian):** The text does not contain relevant details or information about a crisis.\n",
    "\n",
    "Return only the classification label (0, 1, 2, 3, or 4) without any extra text or explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "HUMA_TEXT_ONLY_USER_PROMPT = \"\"\"\n",
    "Classify the text delimited by triple quotes (\\\"\\\"\\\" \\\"\\\"\\\") into one of the following categories:\n",
    "- **0 (affected individuals):** The text provides any relevant information about people impacted by the crisis, including injured, missing, founding, or deceased individuals.\n",
    "- **1 (rescue, volunteering, or donation efforts):** The text provides any relevant information about rescue operations, volunteering, or donations in a crisis.\n",
    "- **2 (infrastructure and utility damage):** The text provides any relevant information about damage to buildings, roads, power lines, other essential utilities, and vehicles in a crisis.\n",
    "- **3 (other relevant information):** The text provides any relevant details about a crisis that do not fit into the categories 0, 1, and 2 above.\n",
    "- **4 (not humanitarian):** The text does not contain relevant details or information about a crisis.\n",
    "    \n",
    "Return only the classification label (0, 1, 2, 3, or 4) without any extra text or explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "HUMA_IMAGE_ONLY_SYSTEM_PROMPT = \"\"\"\n",
    "You are an AI model that classifies image into one of five categories based on its humanitarian relevance in a crisis. Your task is to analyze the given image and assign it the most appropriate category.\n",
    "\n",
    "Your classification should prioritize identifying image that provide useful crisis-related information, even if they are brief.\n",
    "\n",
    "Please note that if the image is not relevant to a crisis, it should be labeled as 'not humanitarian'. On the other hand, if the image is relevant about a crisis (flood, fire, earthquake, storm, hurricane...) then it should be categorized as either 0, 1, 2, or 3. When the image is relevant to a crisis but does not mention to either people, donation or volunteer, and property damage then you can put it into category 3.\n",
    "\n",
    "Classify the given image into one of the following categories:\n",
    "\n",
    "- **0 (affected individuals):** The image provides any relevant information about people impacted by the crisis, including injured, missing, founding, or deceased individuals. In short, if it is about a crisis and related to humans but not related to rescue, volunteering, or donation activities, then it should be labeled as 0.\n",
    "\n",
    "- **1 (rescue, volunteering, or donation efforts):** The image provides any relevant information about rescue operations, volunteering, or donations in a crisis.\n",
    "\n",
    "- **2 (infrastructure and utility damage):** The image provides any relevant information about damage to buildings, roads, power lines, other essential utilities, and vehicles in a crisis. This includes interruptions of utility services or infrastructure during a crisis.\n",
    "\n",
    "- **3 (other relevant information):** The image provides any relevant details about a crisis that do not fit into the categories 0, 1, and 2 above. In short, if it is about a crisis but is not related to human impact, rescue, volunteering, donation activities, or property or vehicle damage, then it should be labeled as category 3.\n",
    "\n",
    "- **4 (not humanitarian):** The image does not contain relevant details or information about a crisis.\n",
    "\n",
    "Return only the classification label (0, 1, 2, 3, or 4) without any extra text or explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "HUMA_IMAGE_ONLY_USER_PROMPT = \"\"\"\n",
    "You are an AI model that classifies image into one of five categories based on its humanitarian relevance in a crisis. Your task is to analyze the given image and assign it the most appropriate category.\n",
    "\n",
    "Your classification should prioritize identifying image that provide useful crisis-related information, even if they are brief.\n",
    "\n",
    "Please note that if the image is not relevant to a crisis, it should be labeled as 'not humanitarian'. On the other hand, if the image is relevant about a crisis (flood, fire, earthquake, storm, hurricane...) then it should be categorized as either 0, 1, 2, or 3. When the image is relevant to a crisis but does not mention to either people, donation or volunteer, and property damage then you can put it into category 3.\n",
    "\n",
    "Classify the given image into one of the following categories:\n",
    "\n",
    "- **0 (affected individuals):** The image provides any relevant information about people impacted by the crisis, including injured, missing, founding, or deceased individuals. In short, if it is about a crisis and related to humans but not related to rescue, volunteering, or donation activities, then it should be labeled as 0.\n",
    "\n",
    "- **1 (rescue, volunteering, or donation efforts):** The image provides any relevant information about rescue operations, volunteering, or donations in a crisis.\n",
    "\n",
    "- **2 (infrastructure and utility damage):** The image provides any relevant information about damage to buildings, roads, power lines, other essential utilities, and vehicles in a crisis. This includes interruptions of utility services or infrastructure during a crisis.\n",
    "\n",
    "- **3 (other relevant information):** The image provides any relevant details about a crisis that do not fit into the categories 0, 1, and 2 above. In short, if it is about a crisis but is not related to human impact, rescue, volunteering, donation activities, or property or vehicle damage, then it should be labeled as category 3.\n",
    "\n",
    "- **4 (not humanitarian):** The image does not contain relevant details or information about a crisis.\n",
    "\n",
    "Return only the classification label (0, 1, 2, 3, or 4) without any extra text or explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "HUMA_TEXT_IMAGE_SYSTEM_PROMPT = \"\"\"You are an AI model that classifies both the text and the image into one of five categories based on its humanitarian relevance in a crisis. Your task is to analyze the given text and image and assign them the most appropriate category.\n",
    "\n",
    "Your classification should prioritize identifying text and image that provide useful crisis-related information, even if they are brief, but you should consider the meaning of the text and image as a whole and should not just focus on individual keywords or hashtags within the text. For example, if the text only contains a hashtag about donation, but the text and the image are not relevant to a crisis, then it should be categorized as 'not humanitarian'.\n",
    "\n",
    "Please note that if the text and the image are not relevant to a crisis, it should be labeled as 'not humanitarian'. On the other hand, if the text and the image mention about a crisis (flood, fire, earthquake, storm, hurricane...) then they should be categorized as either 0, 1, 2, or 3. When the text and the image are relevant to a crisis but do not mention to either people, donation or volunteer, and property damage then you can put them into category 3.\n",
    "\n",
    "Classify the text delimited by triple quotes (\\\"\\\"\\\" \\\"\\\"\\\") and the given image into one of the following categories:\n",
    "\n",
    "- **0 (affected individuals):** The text and the image provide any relevant information about people impacted by the crisis, including injured, missing, founding, or deceased individuals. In short, if they are about a crisis and related to humans but not related to rescue, volunteering, or donation activities, then they should be labeled as 0.\n",
    "\n",
    "- **1 (rescue, volunteering, or donation efforts):** The text and the image provide any relevant information about rescue operations, volunteering, or donations in a crisis.\n",
    "\n",
    "- **2 (infrastructure and utility damage):** The text and the image provide any relevant information about damage to buildings, roads, power lines, other essential utilities, and vehicles in a crisis. This includes interruptions of utility services or infrastructure during a crisis.\n",
    "\n",
    "- **3 (other relevant information):** The text and the image provide any relevant details about a crisis that do not fit into the categories 0, 1, and 2 above. In short, if they are about a crisis but are not related to human impact, rescue, volunteering, donation activities, or property or vehicle damage, then they should be labeled as category 3.\n",
    "\n",
    "- **4 (not humanitarian):** The text and the image do not contain relevant details or information about a crisis.\n",
    "\n",
    "Return only the classification label (0, 1, 2, 3, or 4) without any extra text or explanation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "HUMA_TEXT_IMAGE_USER_PROMPT = \"\"\"\n",
    "Classify the text delimited by triple quotes (\\\"\\\"\\\" \\\"\\\"\\\") and the given image into one of the following categories:\n",
    "- **0 (affected individuals):** The text and the image provide any relevant information about people impacted by the crisis, including injured, missing, founding, or deceased individuals.\n",
    "- **1 (rescue, volunteering, or donation efforts):** The text and the image provide any relevant information about rescue operations, volunteering, or donations in a crisis.\n",
    "- **2 (infrastructure and utility damage):** The text and the image provide any relevant information about damage to buildings, roads, power lines, other essential utilities, and vehicles in a crisis.\n",
    "- **3 (other relevant information):** The text and the image provide any relevant details about a crisis that do not fit into the categories 0, 1, and 2 above.\n",
    "- **4 (not humanitarian):** The text and the image do not contain relevant details or information about a crisis.\n",
    "    \n",
    "Return only the classification label (0, 1, 2, 3, or 4) without any extra text or explanation.\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb0956-3367-43bc-a916-6dec6fc0ef4a",
   "metadata": {},
   "source": [
    "# GPT FEW SHOTS IMPLEMENTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec84d1c2-8788-4798-81f4-25e7e2dc9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set up OpenAI API key\n",
    "client = OpenAI(api_key=\"your-API-key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569a48b9-4ce3-4a5f-bf67-e28e879ea83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import openai\n",
    "import base64\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def ensure_path_exists(file_path):\n",
    "    \"\"\"\n",
    "    Ensure that the directory for the given file path exists. Create it if it doesn't.\n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def update_remaining_time(start_time, current_index, total_tests, milestone_desc):\n",
    "    \"\"\"\n",
    "    Prints an estimated remaining time for the test process.\n",
    "\n",
    "    :param start_time: the time when the testing started (time.time())\n",
    "    :param current_index: current test item index (1-based)\n",
    "    :param total_tests: total number of test items\n",
    "    :param milestone_desc: string describing the milestone, e.g. \"25% of tests\"\n",
    "    \"\"\"\n",
    "    elapsed = time.time() - start_time\n",
    "    tests_done = current_index\n",
    "    if tests_done == 0:\n",
    "        return\n",
    "\n",
    "    tests_left = total_tests - tests_done\n",
    "    # Estimated total time if the average up to now continues\n",
    "    time_per_test = elapsed / tests_done\n",
    "    estimated_remaining = tests_left * time_per_test\n",
    "\n",
    "    mins, secs = divmod(estimated_remaining, 60)\n",
    "    hours, mins = divmod(mins, 60)\n",
    "\n",
    "    print(f\"\\n📊--- Reached {milestone_desc} ---📊\")\n",
    "    print(f\"    Elapsed time: {elapsed:.2f} seconds for {tests_done} items.\")\n",
    "    print(f\"    Estimated time left: {int(hours)}h:{int(mins)}m:{int(secs)}s for {tests_left} items.\\n\")\n",
    "    \n",
    "\n",
    "def get_few_shot_examples(devdata, few_shot_num, consistency, label_key=\"label\"):\n",
    "    \"\"\"\n",
    "    Extract few-shot examples from the development set.\n",
    "    Each label's entry is a list of dicts with just \"text\" and \"image_path\".\n",
    "\n",
    "    :param devdata: a list of dictionaries, each dict containing at least {\"text\", \"label\", \"image_path\"}\n",
    "    :param few_shot_num: how many examples per class to retrieve\n",
    "    :param consistency: if True, the same examples (randomly selected once) \n",
    "                        will be reused for all test items. \n",
    "                        If False, for each test item, we sample again from the dev set.\n",
    "    :param label_key: dictionary key for the label in dev data\n",
    "\n",
    "    :return: a dictionary:\n",
    "       {\n",
    "            label_value_1: [ { 'text': ..., 'image_path': ..., 'label': ... }, ... ],\n",
    "            label_value_2: [...],\n",
    "            ...\n",
    "       }\n",
    "       Each list has up to `few_shot_num` items for that label.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, group dev entries by label.\n",
    "    grouped_by_label = defaultdict(list)\n",
    "    for item in devdata:\n",
    "        lbl = item[label_key]\n",
    "        grouped_by_label[lbl].append({\n",
    "            \"text\": item[\"text\"],\n",
    "            \"image_path\": item[\"image_path\"],\n",
    "            \"label\": lbl \n",
    "        })\n",
    "\n",
    "    # For each label, shuffle once (so that we can consistently pick the top `few_shot_num`).\n",
    "    # We'll store them so that if consistency=True, we reuse them for all test items.\n",
    "    # If consistency=False, we’ll sample anew inside the main loop.\n",
    "    few_shot_pool = {}\n",
    "    for lbl, items in grouped_by_label.items():\n",
    "        random.shuffle(items)\n",
    "        few_shot_pool[lbl] = items[:few_shot_num]\n",
    "\n",
    "    return few_shot_pool\n",
    "\n",
    "def build_few_shot_classification_chat(\n",
    "    system_content,\n",
    "    user_prompt_instructions,\n",
    "    few_shot_examples,\n",
    "    final_text=None,\n",
    "    final_image_path=None,\n",
    "    use_text=True,\n",
    "    use_image=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a chat message list for classification using few-shot examples.\n",
    "    \n",
    "    :param system_content: system message content (str), e.g. \"You are a helpful classifier...\"\n",
    "    :param user_prompt_instructions: the classification instructions (multiline string).\n",
    "    :param few_shot_examples: a list of dicts, each with:\n",
    "         {\n",
    "            \"text\": \"...\",\n",
    "            \"image_path\": \"...\",\n",
    "            \"label\": 0  # or 1,2,3,4\n",
    "         }\n",
    "    :param final_text: The final text you want classified (string).\n",
    "    :param final_image_path: The final image URL or path if you want to classify an image.\n",
    "    :param use_text: Whether to include text in user messages\n",
    "    :param use_image: Whether to include image in user messages\n",
    "\n",
    "    :return: a list of messages (system, user, assistant) to be passed to openai.ChatCompletion.create(...)\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content}\n",
    "    ]\n",
    "\n",
    "    # -- 1) FEW-SHOT EXAMPLES --\n",
    "    for example in few_shot_examples:\n",
    "        # Build user content for the example\n",
    "        user_content_blocks = []\n",
    "\n",
    "        combined_text = f\"{user_prompt_instructions}\\n\\n\"\n",
    "        \n",
    "        # If using text, embed instructions + example text\n",
    "        if use_text:\n",
    "            # Combine instructions + the example text, enclosed in triple quotes\n",
    "            combined_text += (                \n",
    "                f\"\\\"\\\"\\\"{example['text']}\\\"\\\"\\\"\"\n",
    "            )\n",
    "        \n",
    "        user_content_blocks.append({\"type\": \"text\", \"text\": combined_text})\n",
    "\n",
    "        # If using images, add the image to the user message        \n",
    "        if use_image:\n",
    "            # Getting the base64 string\n",
    "            base64_image = encode_image(example[\"image_path\"]) \n",
    "            \n",
    "            user_content_blocks.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",      \n",
    "                    \"image_url\":  {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # If neither text nor image is used, ensure not to have an empty list\n",
    "        if not user_content_blocks:\n",
    "            user_content_blocks.append({\"type\": \"text\", \"text\": \"\"})\n",
    "\n",
    "        # Add user message for the example\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content_blocks})\n",
    "\n",
    "        # Add assistant message with the correct label\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": str(example[\"label\"])  # just 0, 1, etc.\n",
    "        })\n",
    "\n",
    "    # -- 2) FINAL PROMPT FOR CLASSIFICATION --\n",
    "    final_user_blocks = []\n",
    "\n",
    "    final_user_text = f\"{user_prompt_instructions}\\n\\n\"\n",
    "    \n",
    "    if use_text and final_text:\n",
    "        final_user_text += (            \n",
    "            f\"\\\"\\\"\\\"{final_text}\\\"\\\"\\\"\"\n",
    "        )\n",
    "    \n",
    "    final_user_blocks.append({\"type\": \"text\", \"text\": final_user_text})\n",
    "\n",
    "    if use_image and final_image_path:\n",
    "        # Getting the base64 string\n",
    "        base64_image = encode_image(final_image_path) \n",
    "        \n",
    "        final_user_blocks.append(\n",
    "            {\n",
    "                \"type\": \"image_url\",      \n",
    "                \"image_url\":  {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not final_user_blocks:\n",
    "        final_user_blocks.append({\"type\": \"text\", \"text\": \"\"})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": final_user_blocks})\n",
    "\n",
    "    return messages\n",
    "\n",
    "\n",
    "def test_gpt(\n",
    "    model_name,\n",
    "    test_data_path,\n",
    "    dev_data_path,\n",
    "    result_path,\n",
    "    system_prompt,\n",
    "    user_prompt,\n",
    "    few_shot_num=3,\n",
    "    consistency=True,\n",
    "    use_text=True,\n",
    "    use_image=True,\n",
    "    label_key=\"label\"\n",
    "):\n",
    "    \"\"\"\n",
    "    A high-level function to test classification using the above logic.\n",
    "    - Reads dev + test data\n",
    "    - Extracts few-shot examples\n",
    "    - For each test item, builds the chat and calls the OpenAI ChatCompletion\n",
    "    - Saves results to a JSON file\n",
    "\n",
    "    :param model_name: e.g. \"gpt-3.5-turbo\", \"gpt-4\"\n",
    "    :param test_data_path: Path to test data (JSON)\n",
    "    :param dev_data_path: Path to dev data (JSON)\n",
    "    :param result_path: Where to save the classification results\n",
    "    :param system_prompt: The system message\n",
    "    :param user_prompt: The classification instructions\n",
    "    :param few_shot_num: How many examples per label to pull from dev set\n",
    "    :param consistency: If True, use the same few-shot examples for all test items\n",
    "                        If False, re-sample from dev data for each test item\n",
    "    :param use_text: Whether to use text in the user messages\n",
    "    :param use_image: Whether to use images in the user messages\n",
    "    :param label_key: The key to look up the label in the dev/test data\n",
    "    \"\"\"\n",
    "\n",
    "    ensure_path_exists(result_path)\n",
    "\n",
    "    # Load dev + test data\n",
    "    with open(dev_data_path, \"r\", encoding=\"utf-8\") as f_dev:\n",
    "        devdata = json.load(f_dev)\n",
    "    with open(test_data_path, \"r\", encoding=\"utf-8\") as f_test:\n",
    "        testdata = json.load(f_test)\n",
    "\n",
    "    # If consistency=True, gather the few-shot examples once\n",
    "    # If consistency=False, we'll do it inside the test loop\n",
    "    few_shot_pool = None\n",
    "    if few_shot_num > 0:\n",
    "        few_shot_pool = get_few_shot_examples(devdata, few_shot_num, consistency, label_key)\n",
    "\n",
    "    max_requests_per_minute = 9000\n",
    "    request_interval = 60  # seconds\n",
    "    request_count = 0\n",
    "    retry_limit = 5\n",
    "\n",
    "    valid_labels = {0, 1, 2, 3, 4}\n",
    "\n",
    "    results = []\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "\n",
    "    total_tests = len(testdata)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for index, entry in enumerate(testdata):\n",
    "        text = entry[\"text\"]\n",
    "        image_path = entry[\"image_path\"]\n",
    "        y_true = entry[label_key]  # ground truth label\n",
    "\n",
    "        # Time tracking at checkpoints\n",
    "        if index == (total_tests // 100) and total_tests >= 100:\n",
    "            update_remaining_time(start_time, index + 1, total_tests, \"1% of tests\")\n",
    "        elif index == (total_tests // 4):\n",
    "            update_remaining_time(start_time, index + 1, total_tests, \"25% of tests\")\n",
    "        elif index == (total_tests // 2):\n",
    "            update_remaining_time(start_time, index + 1, total_tests, \"50% of tests\")\n",
    "        elif index == ((3 * total_tests) // 4):\n",
    "            update_remaining_time(start_time, index + 1, total_tests, \"75% of tests\")\n",
    "\n",
    "        # If consistency=False, re-sample the few_shot_pool for each item\n",
    "        if few_shot_num > 0 and not consistency:\n",
    "            # re-run get_few_shot_examples for each test instance\n",
    "            few_shot_pool = get_few_shot_examples(devdata, few_shot_num, consistency, label_key)\n",
    "\n",
    "        # Flatten out the few_shot_pool dictionary into a single list\n",
    "        # so that build_few_shot_classification_chat can iterate easily\n",
    "        all_examples = []\n",
    "        if few_shot_pool:\n",
    "            for lbl, example_list in few_shot_pool.items():\n",
    "                all_examples.extend(example_list)\n",
    "\n",
    "        # Build the chat\n",
    "        messages = build_few_shot_classification_chat(\n",
    "            system_content=system_prompt,\n",
    "            user_prompt_instructions=user_prompt,\n",
    "            few_shot_examples=all_examples,\n",
    "            final_text=text,\n",
    "            final_image_path=image_path,\n",
    "            use_text=use_text,\n",
    "            use_image=use_image\n",
    "        )\n",
    "\n",
    "        # Send request\n",
    "        y_pred = None\n",
    "        retry_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=messages\n",
    "                )                \n",
    "                answer = response.choices[0].message.content.strip()\n",
    "                \n",
    "                # Attempt to parse it as an int\n",
    "                if answer.isdigit():\n",
    "                    predicted_label = int(answer)\n",
    "                    if predicted_label in valid_labels:\n",
    "                        y_pred = predicted_label\n",
    "                        break\n",
    "                \n",
    "                # Not a valid integer\n",
    "                retry_count += 1\n",
    "                if retry_count >= retry_limit:\n",
    "                    print(f\"❗ Invalid label '{answer}' after {retry_count} tries. Skipping.\")\n",
    "                    y_pred = None\n",
    "                    break\n",
    "                print(f\"Invalid label '{answer}', retrying in 3 sec...\")\n",
    "                time.sleep(3)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error on request: {e}\")\n",
    "                retry_count += 1\n",
    "                if retry_count >= retry_limit:\n",
    "                    print(f\"Too many errors ({retry_count}). Skipping.\")\n",
    "                    break\n",
    "                time.sleep(3)\n",
    "\n",
    "        # If we never got a valid label, skip\n",
    "        if y_pred is None:\n",
    "            print(f\"\\nBad result! Ignoring test case {index+1}.\\n\")\n",
    "            continue        \n",
    "\n",
    "        # Save result for this item\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"image_path\": image_path,\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred\": y_pred\n",
    "        })\n",
    "\n",
    "        y_true_list.append(y_true)\n",
    "        y_pred_list.append(y_pred)\n",
    "\n",
    "        # Rate-limit control\n",
    "        request_count += 1\n",
    "        if request_count >= max_requests_per_minute:\n",
    "            print(f\"Reached {request_count} requests, sleeping {request_interval} seconds...\")\n",
    "            time.sleep(request_interval)\n",
    "            request_count = 0\n",
    "\n",
    "    # Write results to file\n",
    "    with open(result_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(results, f_out, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nResults saved to {result_path}\\n\")\n",
    "\n",
    "        # Calculate statistics\n",
    "    if len(y_true_list) > 0:\n",
    "        accuracy = accuracy_score(y_true_list, y_pred_list)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_list,\n",
    "            y_pred_list,\n",
    "            average=\"weighted\"\n",
    "        )\n",
    "        print(\"\\n📊 **Test Statistics**\")\n",
    "        print(f\"✔ Accuracy:    {accuracy:.4f}\")\n",
    "        print(f\"✔ Precision:   {precision:.4f}\")\n",
    "        print(f\"✔ Recall:      {recall:.4f}\")\n",
    "        print(f\"✔ F1 Score:    {f1:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo valid predictions were generated.\")\n",
    "\n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(f\"\\n✅ Done testing! Elapsed time: {elapsed_total:.2f} seconds total.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4a8d2-40a6-4034-8126-624c5e6ea23f",
   "metadata": {},
   "source": [
    "# GPT ZERO SHOT - HUMANITARIAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549b9e7-8e19-463b-ad9e-54f7604eca06",
   "metadata": {},
   "source": [
    "## Text Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69fcf6-35f3-4284-ab65-54183038ffbf",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8131818-9f26-4383-bd95-8370b0242bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 8.23 seconds for 10 items.\n",
      "    Estimated time left: 0h:12m:58s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 130.43 seconds for 239 items.\n",
      "    Estimated time left: 0h:6m:30s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 230.63 seconds for 478 items.\n",
      "    Estimated time left: 0h:3m:50s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 345.08 seconds for 717 items.\n",
      "    Estimated time left: 0h:1m:54s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-Zeroshot-Text-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7466\n",
      "✔ Precision:   0.7503\n",
      "✔ Recall:      0.7466\n",
      "✔ F1 Score:    0.7427\n",
      "\n",
      "✅ Done testing! Elapsed time: 446.31 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-Zeroshot-Text-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_ONLY_USER_PROMPT,\n",
    "    few_shot_num=0,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=False, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f5f54b-2cd6-4e24-8160-3de6b5de3e32",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f91c3b-8ae7-4c60-91a9-5fb7b4f50791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 3.67 seconds for 10 items.\n",
      "    Estimated time left: 0h:5m:47s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 112.31 seconds for 239 items.\n",
      "    Estimated time left: 0h:5m:36s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 220.46 seconds for 478 items.\n",
      "    Estimated time left: 0h:3m:40s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 325.27 seconds for 717 items.\n",
      "    Estimated time left: 0h:1m:47s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-Zeroshot-Text-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7424\n",
      "✔ Precision:   0.7797\n",
      "✔ Recall:      0.7424\n",
      "✔ F1 Score:    0.7507\n",
      "\n",
      "✅ Done testing! Elapsed time: 449.56 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-Zeroshot-Text-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_ONLY_USER_PROMPT,\n",
    "    few_shot_num=0,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=False, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db28a4-6840-44c5-a5fb-ed5d21433930",
   "metadata": {},
   "source": [
    "### Image Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4aeac3-5a57-476a-8543-21e3475f935b",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88f58fe4-0831-4941-bf81-1e73173b107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 22.85 seconds for 10 items.\n",
      "    Estimated time left: 0h:35m:59s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 551.38 seconds for 239 items.\n",
      "    Estimated time left: 0h:27m:31s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 1120.52 seconds for 478 items.\n",
      "    Estimated time left: 0h:18m:38s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 1669.36 seconds for 717 items.\n",
      "    Estimated time left: 0h:9m:14s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-Zeroshot-Image-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.8471\n",
      "✔ Precision:   0.8718\n",
      "✔ Recall:      0.8471\n",
      "✔ F1 Score:    0.8528\n",
      "\n",
      "✅ Done testing! Elapsed time: 2300.83 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-Zeroshot-Image-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_IMAGE_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_IMAGE_ONLY_USER_PROMPT,\n",
    "    few_shot_num=0,\n",
    "    consistency=True,\n",
    "    use_text=False,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0af417-1af8-4069-b7df-e7cdf091fb23",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38ce0941-c9c8-47cf-8c51-0510a952dea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 35.92 seconds for 10 items.\n",
      "    Estimated time left: 0h:56m:34s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 564.03 seconds for 239 items.\n",
      "    Estimated time left: 0h:28m:9s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 1113.14 seconds for 478 items.\n",
      "    Estimated time left: 0h:18m:30s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 1659.99 seconds for 717 items.\n",
      "    Estimated time left: 0h:9m:11s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-Zeroshot-Image-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.8450\n",
      "✔ Precision:   0.8658\n",
      "✔ Recall:      0.8450\n",
      "✔ F1 Score:    0.8512\n",
      "\n",
      "✅ Done testing! Elapsed time: 2243.55 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-Zeroshot-Image-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_IMAGE_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_IMAGE_ONLY_USER_PROMPT,\n",
    "    few_shot_num=0,\n",
    "    consistency=True,\n",
    "    use_text=False,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949270e1-d78d-43e2-9c11-f92615c2fa4f",
   "metadata": {},
   "source": [
    "### Text Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1542b-5154-48c8-95f0-ec7596ea61f9",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c24e07a-04c1-44d7-9297-8eb3fcb2496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 25.51 seconds for 10 items.\n",
      "    Estimated time left: 0h:40m:11s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 562.82 seconds for 239 items.\n",
      "    Estimated time left: 0h:28m:6s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 1236.38 seconds for 478 items.\n",
      "    Estimated time left: 0h:20m:33s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 2447.86 seconds for 717 items.\n",
      "    Estimated time left: 0h:13m:32s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-Zeroshot-Text-Image.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7508\n",
      "✔ Precision:   0.8177\n",
      "✔ Recall:      0.7508\n",
      "✔ F1 Score:    0.7552\n",
      "\n",
      "✅ Done testing! Elapsed time: 3190.38 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-Zeroshot-Text-Image.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_IMAGE_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_IMAGE_USER_PROMPT,\n",
    "    few_shot_num=0,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5599315-5304-432e-81db-e38c1672e839",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab40f5e-8fb1-4b1d-8301-d3a2540dcab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 19.50 seconds for 10 items.\n",
      "    Estimated time left: 0h:30m:43s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 539.85 seconds for 239 items.\n",
      "    Estimated time left: 0h:26m:57s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 1060.73 seconds for 478 items.\n",
      "    Estimated time left: 0h:17m:38s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 1561.35 seconds for 717 items.\n",
      "    Estimated time left: 0h:8m:38s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-Zeroshot-Text-Image.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7717\n",
      "✔ Precision:   0.8348\n",
      "✔ Recall:      0.7717\n",
      "✔ F1 Score:    0.7859\n",
      "\n",
      "✅ Done testing! Elapsed time: 2023.24 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-Zeroshot-Text-Image.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_IMAGE_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_IMAGE_USER_PROMPT,\n",
    "    few_shot_num=0,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a7c318-b1c8-439b-b442-6d81e5e4792a",
   "metadata": {},
   "source": [
    "# GPT ONE SHOT - HUMANITARIAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bcb1a5-0f74-4ccd-a556-a430f709f5e1",
   "metadata": {},
   "source": [
    "### Text Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28f4bd-4e92-4df4-aca6-9ef811e919f5",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a98b0ab7-a2c6-4059-a9af-fdb5126f97e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 5.58 seconds for 10 items.\n",
      "    Estimated time left: 0h:8m:47s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 125.80 seconds for 239 items.\n",
      "    Estimated time left: 0h:6m:16s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 270.44 seconds for 478 items.\n",
      "    Estimated time left: 0h:4m:29s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 388.93 seconds for 717 items.\n",
      "    Estimated time left: 0h:2m:9s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-Oneshot-Text-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7110\n",
      "✔ Precision:   0.7659\n",
      "✔ Recall:      0.7110\n",
      "✔ F1 Score:    0.7200\n",
      "\n",
      "✅ Done testing! Elapsed time: 500.96 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-Oneshot-Text-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_ONLY_USER_PROMPT,\n",
    "    few_shot_num=1,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=False, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc85a81-eca0-47dc-baad-6db60c6900b9",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07494c7a-8d6f-438e-920d-cf2c90059cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 3.90 seconds for 10 items.\n",
      "    Estimated time left: 0h:6m:8s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 120.19 seconds for 239 items.\n",
      "    Estimated time left: 0h:6m:0s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 235.76 seconds for 478 items.\n",
      "    Estimated time left: 0h:3m:55s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 334.21 seconds for 717 items.\n",
      "    Estimated time left: 0h:1m:50s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-Oneshot-Text-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.6995\n",
      "✔ Precision:   0.7764\n",
      "✔ Recall:      0.6995\n",
      "✔ F1 Score:    0.7089\n",
      "\n",
      "✅ Done testing! Elapsed time: 438.36 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-Oneshot-Text-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_ONLY_USER_PROMPT,\n",
    "    few_shot_num=1,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=False, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e6c9e-37b4-4159-9811-e7178d701443",
   "metadata": {},
   "source": [
    "### Image Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eedc7e2-8a65-490c-b6db-d29ad5c8da0a",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6584ae7-3122-4ff6-8205-c78b5f282060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 47.89 seconds for 10 items.\n",
      "    Estimated time left: 1h:15m:25s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 1109.56 seconds for 239 items.\n",
      "    Estimated time left: 0h:55m:24s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 2234.35 seconds for 478 items.\n",
      "    Estimated time left: 0h:37m:9s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 3353.19 seconds for 717 items.\n",
      "    Estimated time left: 0h:18m:33s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-Oneshot-Image-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.8429\n",
      "✔ Precision:   0.8716\n",
      "✔ Recall:      0.8429\n",
      "✔ F1 Score:    0.8504\n",
      "\n",
      "✅ Done testing! Elapsed time: 4681.35 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-Oneshot-Image-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_IMAGE_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_IMAGE_ONLY_USER_PROMPT,\n",
    "    few_shot_num=1,\n",
    "    consistency=True,\n",
    "    use_text=False,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb0518-40da-439e-8042-0a68b2f7448d",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f522875-c63e-4f50-aa42-5b1bfd97bfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 19.87 seconds for 10 items.\n",
      "    Estimated time left: 0h:31m:17s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 617.49 seconds for 239 items.\n",
      "    Estimated time left: 0h:30m:49s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 1814.08 seconds for 478 items.\n",
      "    Estimated time left: 0h:30m:10s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 2559.19 seconds for 717 items.\n",
      "    Estimated time left: 0h:14m:9s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-Oneshot-Image-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.8450\n",
      "✔ Precision:   0.8668\n",
      "✔ Recall:      0.8450\n",
      "✔ F1 Score:    0.8513\n",
      "\n",
      "✅ Done testing! Elapsed time: 3128.21 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-Oneshot-Image-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_IMAGE_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_IMAGE_ONLY_USER_PROMPT,\n",
    "    few_shot_num=0,\n",
    "    consistency=True,\n",
    "    use_text=False,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28235b4-209c-4be4-bc3b-0d181bbf0c3f",
   "metadata": {},
   "source": [
    "### Text Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87a775-5da3-46b1-b871-c5f922d27162",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba5e9658-84b6-40fc-a901-100c216b8e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 45.02 seconds for 10 items.\n",
      "    Estimated time left: 1h:10m:54s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 1256.25 seconds for 239 items.\n",
      "    Estimated time left: 1h:2m:43s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 2556.89 seconds for 478 items.\n",
      "    Estimated time left: 0h:42m:31s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 3816.23 seconds for 717 items.\n",
      "    Estimated time left: 0h:21m:6s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-Oneshot-Text-Image.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7529\n",
      "✔ Precision:   0.8247\n",
      "✔ Recall:      0.7529\n",
      "✔ F1 Score:    0.7597\n",
      "\n",
      "✅ Done testing! Elapsed time: 5283.80 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-Oneshot-Text-Image.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_IMAGE_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_IMAGE_USER_PROMPT,\n",
    "    few_shot_num=1,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6604295-e024-45a0-9bf9-68d2fffaca7c",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85b75e92-c778-424f-b3e7-be3f01d77c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 50.14 seconds for 10 items.\n",
      "    Estimated time left: 1h:18m:58s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 1371.73 seconds for 239 items.\n",
      "    Estimated time left: 1h:8m:29s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 2768.77 seconds for 478 items.\n",
      "    Estimated time left: 0h:46m:2s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 4128.14 seconds for 717 items.\n",
      "    Estimated time left: 0h:22m:50s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-Oneshot-Text-Image.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7874\n",
      "✔ Precision:   0.8434\n",
      "✔ Recall:      0.7874\n",
      "✔ F1 Score:    0.8026\n",
      "\n",
      "✅ Done testing! Elapsed time: 5500.95 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-Oneshot-Text-Image.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_IMAGE_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_IMAGE_USER_PROMPT,\n",
    "    few_shot_num=1,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9408f-319d-40a4-9995-db99d6804470",
   "metadata": {},
   "source": [
    "# GPT 5 SHOTS - HUMANITARIAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611a7dc-2949-4221-a7ae-744634a11343",
   "metadata": {},
   "source": [
    "### Text Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad45fe-0901-4877-aa06-db9135080ee5",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab6c57d7-d29c-4721-8770-882fbe845ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 9.41 seconds for 10 items.\n",
      "    Estimated time left: 0h:14m:49s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 235.08 seconds for 239 items.\n",
      "    Estimated time left: 0h:11m:44s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 470.45 seconds for 478 items.\n",
      "    Estimated time left: 0h:7m:49s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 704.85 seconds for 717 items.\n",
      "    Estimated time left: 0h:3m:53s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-5shot-Inconsistent-Text-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7037\n",
      "✔ Precision:   0.7625\n",
      "✔ Recall:      0.7037\n",
      "✔ F1 Score:    0.7163\n",
      "\n",
      "✅ Done testing! Elapsed time: 962.24 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-5shot-Inconsistent-Text-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_ONLY_USER_PROMPT,\n",
    "    few_shot_num=5,\n",
    "    consistency=False,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=False, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764cbe1b-7772-4766-bc08-43ef95bd0260",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab42008b-d329-4864-9f8a-1a59b45e78ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 7.66 seconds for 10 items.\n",
      "    Estimated time left: 0h:12m:3s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 175.27 seconds for 239 items.\n",
      "    Estimated time left: 0h:8m:45s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 358.72 seconds for 478 items.\n",
      "    Estimated time left: 0h:5m:57s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 528.89 seconds for 717 items.\n",
      "    Estimated time left: 0h:2m:55s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-5shot-Text-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7131\n",
      "✔ Precision:   0.7780\n",
      "✔ Recall:      0.7131\n",
      "✔ F1 Score:    0.7276\n",
      "\n",
      "✅ Done testing! Elapsed time: 684.41 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-5shot-Text-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_ONLY_USER_PROMPT,\n",
    "    few_shot_num=5,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=False, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3232d-3f0c-4432-83f7-2b89997d77d7",
   "metadata": {},
   "source": [
    "### Image Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6018131-a83e-493e-99c2-f23bb2f4c500",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "718c1a14-7913-4b8c-936e-8642937b1ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 156.79 seconds for 10 items.\n",
      "    Estimated time left: 4h:6m:56s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 4394.19 seconds for 239 items.\n",
      "    Estimated time left: 3h:39m:24s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 8692.83 seconds for 478 items.\n",
      "    Estimated time left: 2h:24m:34s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 13096.36 seconds for 717 items.\n",
      "    Estimated time left: 1h:12m:27s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-5shot-Image-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.8398\n",
      "✔ Precision:   0.8712\n",
      "✔ Recall:      0.8398\n",
      "✔ F1 Score:    0.8453\n",
      "\n",
      "✅ Done testing! Elapsed time: 17397.49 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-5shot-Image-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_IMAGE_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_IMAGE_ONLY_USER_PROMPT,\n",
    "    few_shot_num=5,\n",
    "    consistency=True,\n",
    "    use_text=False,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ae02c-7bf7-4f04-a127-1e396c4a233e",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "337d5f5b-8d86-4fe5-8f5c-c1c198f794cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 169.17 seconds for 10 items.\n",
      "    Estimated time left: 4h:26m:26s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 3684.66 seconds for 239 items.\n",
      "    Estimated time left: 3h:3m:58s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 7250.21 seconds for 478 items.\n",
      "    Estimated time left: 2h:0m:35s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 10790.71 seconds for 717 items.\n",
      "    Estimated time left: 0h:59m:41s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-5shot-Image-Only.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.8471\n",
      "✔ Precision:   0.8683\n",
      "✔ Recall:      0.8471\n",
      "✔ F1 Score:    0.8546\n",
      "\n",
      "✅ Done testing! Elapsed time: 14260.12 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-5shot-Image-Only.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_IMAGE_ONLY_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_IMAGE_ONLY_USER_PROMPT,\n",
    "    few_shot_num=5,\n",
    "    consistency=True,\n",
    "    use_text=False,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4bc080-958d-45fe-a017-4e71b331f681",
   "metadata": {},
   "source": [
    "### Text Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6447d921-cb02-404a-810b-96da9d54f2f0",
   "metadata": {},
   "source": [
    "##### GPT 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d9b5d0-bb19-490d-b8e4-3c49a33455bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 136.65 seconds for 10 items.\n",
      "    Estimated time left: 3h:35m:13s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 3673.47 seconds for 239 items.\n",
      "    Estimated time left: 3h:3m:25s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 7377.93 seconds for 478 items.\n",
      "    Estimated time left: 2h:2m:42s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 11217.04 seconds for 717 items.\n",
      "    Estimated time left: 1h:2m:3s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-5shot-Text-Image.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7801\n",
      "✔ Precision:   0.8446\n",
      "✔ Recall:      0.7801\n",
      "✔ F1 Score:    0.7902\n",
      "\n",
      "✅ Done testing! Elapsed time: 14904.04 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-5shot-Text-Image.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-2024-08-06\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_IMAGE_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_IMAGE_USER_PROMPT,\n",
    "    few_shot_num=5,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f07f62-1aac-47dd-85f2-271ee283696a",
   "metadata": {},
   "source": [
    "##### GPT 4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f976c5fe-31d7-442c-af61-848f27e59618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊--- Reached 1% of tests ---📊\n",
      "    Elapsed time: 137.44 seconds for 10 items.\n",
      "    Estimated time left: 3h:36m:27s for 945 items.\n",
      "\n",
      "\n",
      "📊--- Reached 25% of tests ---📊\n",
      "    Elapsed time: 3748.99 seconds for 239 items.\n",
      "    Estimated time left: 3h:7m:11s for 716 items.\n",
      "\n",
      "\n",
      "📊--- Reached 50% of tests ---📊\n",
      "    Elapsed time: 7470.85 seconds for 478 items.\n",
      "    Estimated time left: 2h:4m:15s for 477 items.\n",
      "\n",
      "\n",
      "📊--- Reached 75% of tests ---📊\n",
      "    Elapsed time: 11252.60 seconds for 717 items.\n",
      "    Estimated time left: 1h:2m:15s for 238 items.\n",
      "\n",
      "\n",
      "Results saved to test_results/Humanitarian/Test-17/gpt-4o-mini-5shot-Text-Image.json\n",
      "\n",
      "\n",
      "📊 **Test Statistics**\n",
      "✔ Accuracy:    0.7791\n",
      "✔ Precision:   0.8209\n",
      "✔ Recall:      0.7791\n",
      "✔ F1 Score:    0.7863\n",
      "\n",
      "✅ Done testing! Elapsed time: 15050.35 seconds total.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_data_path = \"dataset/Humanitarian/huma_text_image_dev.json\"\n",
    "test_data_path = \"dataset/Humanitarian/huma_text_image_test.json\"\n",
    "result_json_path = \"test_results/Humanitarian/Test-17/gpt-4o-mini-5shot-Text-Image.json\"\n",
    "\n",
    "test_gpt(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    test_data_path=test_data_path,\n",
    "    dev_data_path=dev_data_path,\n",
    "    result_path=result_json_path,\n",
    "    system_prompt=HUMA_TEXT_IMAGE_SYSTEM_PROMPT,\n",
    "    user_prompt=HUMA_TEXT_IMAGE_USER_PROMPT,\n",
    "    few_shot_num=5,\n",
    "    consistency=True,\n",
    "    use_text=True,   # or False if you want image-only\n",
    "    use_image=True, # or True if you have images to pass\n",
    "    label_key=\"label\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec6f84-1648-4f4b-aa9f-f2dbb2b79d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
